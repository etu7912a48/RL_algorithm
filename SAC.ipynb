{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import copy\n",
    "import os\n",
    "from datetime import date\n",
    "import logging\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REPLAY_BUFFER(object):\n",
    "    def __init__(self, capacity, batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.is_enough = False\n",
    "\n",
    "    def add_data(self, data):\n",
    "        self.memory.append(data)\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            self.is_enough =True\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch_index = tf.random.uniform(shape=(self.batch_size, ), minval=0, maxval=self.capacity-1, dtype=tf.dtypes.int32)\n",
    "        return np.array([self.memory[x] for x in batch_index])\n",
    "\n",
    "    def clear_memory (self):\n",
    "        self.memory.clear()\n",
    "        self.is_enough = False\n",
    "\n",
    "# critic net\n",
    "class CRITIC_NET(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=1)\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs,  training=False):\n",
    "        concat = self.concat([inputs[0], inputs[1]])\n",
    "        x = self.dense1(concat)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "# actor net\n",
    "class ACTOR_NET(tf.keras.Model):\n",
    "    def __init__(self, action_dims, clip_min=-1, clip_max=1, epsilon=1e-16):\n",
    "        super().__init__()\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "        self.action_dims = action_dims\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        #mlp\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)\n",
    "\n",
    "        # mean and std\n",
    "        self.mean_dense = tf.keras.layers.Dense(self.action_dims)\n",
    "        self.std_dense = tf.keras.layers.Dense(self.action_dims)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        y = self.mean_dense(x)\n",
    "        z = self.std_dense(x)\n",
    "        return y, z\n",
    "\n",
    "    def eval(self, inputs):\n",
    "        mean, log_std = self.call(inputs)\n",
    "        # you can put limitation on mean and log_std if you want training more stable\n",
    "        # mean = tf.clip_by_value(mean, self.clip_min, self.clip_max)\n",
    "        # log_std = tf.clip_by_value(log_prob, self.clip_min, self.clip_max)\n",
    "\n",
    "        # reparameterization\n",
    "        std = tf.math.exp(log_std)\n",
    "        n_dist = tfp.distributions.Normal(loc=mean, scale=std)\n",
    "        z = mean + std*tf.random.normal(shape=(std.shape))\n",
    "\n",
    "        actions = tf.tanh(z)\n",
    "        log_prob = tf.reduce_sum(n_dist.log_prob(z) - tf.math.log(1 - tf.pow(tf.tanh(z), 2) + self.epsilon), 1, keepdims=True)\n",
    "        return actions, log_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\envs\\registration.py:601: UserWarning: \u001b[33mWARN: Using the latest versioned environment `Ant-v4` instead of the unversioned environment `Ant`.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\wrappers\\record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\113403\\project\\DQL\\SAC_Ant\\20220902 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "game_name = 'Ant'\n",
    "ROOT_PATH = os.getcwd()\n",
    "d = date.today().strftime('%Y%m%d')\n",
    "path = os.path.join(ROOT_PATH, f'SAC_{game_name}', d)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# set logger\n",
    "logger = logging.Logger(f'SAC_{game_name}')\n",
    "fh = logging.FileHandler(filename=f'{path}/misc.log')\n",
    "fh.setFormatter(logging.Formatter(f'[%(name)s] %(asctime)s: %(message)s'))\n",
    "logger.addHandler(fh)\n",
    "logger.info(f'logger has been created')\n",
    "\n",
    "# set hyper parameters\n",
    "GAMMA = 0.99 # discounting factor\n",
    "TAU = 0.003 # soft update\n",
    "\n",
    "# create replay buffer\n",
    "replay_buffer = REPLAY_BUFFER(10000)\n",
    "\n",
    "# create env of the gam\n",
    "env = gym.make(game_name)\n",
    "env = gym.wrappers.RecordVideo(env=env, video_folder=path, episode_trigger=lambda x: x%50==0, name_prefix=f'SAC_{game_name}')\n",
    "\n",
    "# you should change the items below to fit your game\n",
    "# In this code, the parameters below are designed for Ant\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "upper_boundary = env.action_space.high[0]\n",
    "lower_boundary = env.action_space.low[0]\n",
    "\n",
    "# create networks including critic network and actor work\n",
    "actor_net =ACTOR_NET(action_dims=action_space, clip_min=-20, clip_max=20)\n",
    "critic1_net = CRITIC_NET()\n",
    "critic1_target_net = copy.deepcopy(critic1_net)\n",
    "critic2_net = CRITIC_NET()\n",
    "critic2_target_net = copy.deepcopy(critic2_net)\n",
    "\n",
    "# create loss and opt\n",
    "c_loss = tf.keras.losses.MeanSquaredError()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "# create temperature factor alpha\n",
    "alpha = tf.Variable(0.0, dtype=tf.dtypes.float32)\n",
    "h0 = tf.constant(-action_space, dtype=tf.dtypes.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment Ant-v4 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
      "  logger.deprecation(\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\core.py:51: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "logger.info('start to train')\n",
    "replay_buffer.clear_memory()\n",
    "best_reward = 0\n",
    "for episode in range(1000):\n",
    "    # start a new game\n",
    "    observation, info = env.reset(return_info=True)\n",
    "    reward_list = []\n",
    "\n",
    "    for step in range(1000):\n",
    "        observation = observation.reshape(1, state_space)\n",
    "        action, _ = actor_net.eval(observation)\n",
    "        action = tf.clip_by_value(t=action, clip_value_min=lower_boundary, clip_value_max=upper_boundary)\n",
    "        action = tf.squeeze(action)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        # store data into the replay buffer\n",
    "        done = tf.cast(done, tf.float32)\n",
    "        replay_buffer.add_data(np.hstack((tf.squeeze(observation), action, reward, done, tf.squeeze(new_observation))))\n",
    "\n",
    "        # start to training if there are enough data in replay buffer\n",
    "        if replay_buffer.is_enough:\n",
    "            batches = replay_buffer.sample_batch()\n",
    "            states = batches[:, 0:state_space]\n",
    "            actions = batches[:, state_space:(state_space+action_space)]\n",
    "            rewards = batches[:, (state_space+action_space):(state_space+action_space+1)]\n",
    "            dones = batches[:, (state_space+action_space+1):(state_space+action_space+2)]\n",
    "            new_states = batches[:, (state_space+action_space+2):]\n",
    "\n",
    "\n",
    "            # calculate thee gradient of critic net\n",
    "            with tf.GradientTape() as tape:\n",
    "                q1_values = critic1_net((states, actions))\n",
    "                next_actions, log_prob = actor_net.eval(new_states)\n",
    "\n",
    "                q1_target_values = critic1_target_net((new_states, next_actions))\n",
    "                q2_target_values = critic2_target_net((new_states, next_actions))\n",
    "\n",
    "                v = tf.minimum(q1_target_values, q2_target_values)-alpha*log_prob\n",
    "                y = rewards + GAMMA*(1-dones)*v\n",
    "                c1_l = tf.reduce_mean(0.5*c_loss(q1_values, y))\n",
    "            c1_grads = tape.gradient(c1_l, critic1_net.trainable_weights)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q2_values = critic2_net((states, actions))\n",
    "                next_actions, log_prob = actor_net.eval(new_states)\n",
    "\n",
    "                q1_target_values = critic1_target_net((new_states, next_actions))\n",
    "                q2_target_values = critic2_target_net((new_states, next_actions))\n",
    "\n",
    "                v = tf.minimum(q1_target_values, q2_target_values)-alpha*log_prob\n",
    "                y = rewards + GAMMA*(1-dones)*v\n",
    "                c2_l = tf.reduce_mean(0.5*c_loss(q2_values, y))\n",
    "            c2_grads = tape.gradient(c2_l, critic2_net.trainable_weights)\n",
    "\n",
    "            opt.apply_gradients(zip(c1_grads, critic1_net.trainable_weights))\n",
    "            opt.apply_gradients(zip(c2_grads, critic2_net.trainable_weights))\n",
    "\n",
    "            # calculate the gradient of the actor net\n",
    "            with tf.GradientTape() as tape:\n",
    "                current_actions, log_prob = actor_net.eval(states)\n",
    "                current_q1_values = critic1_net((states, current_actions))\n",
    "                current_q2_values = critic2_net((states, current_actions))\n",
    "\n",
    "                min_current_q_values = tf.minimum(current_q1_values, current_q2_values)\n",
    "                soft_q_values = alpha*log_prob - min_current_q_values\n",
    "                a_l = tf.reduce_mean(soft_q_values)\n",
    "            a_grads = tape.gradient(a_l, actor_net.trainable_weights)\n",
    "            opt.apply_gradients(zip(a_grads, actor_net.trainable_weights))\n",
    "\n",
    "            # calculate the gradient of alpha\n",
    "            with tf.GradientTape() as tape:\n",
    "                _, log_prob = actor_net.eval(states)\n",
    "                alpha_l = -tf.reduce_mean(alpha*(log_prob+h0))\n",
    "            alpha_grads = tape.gradient(alpha_l, [alpha])\n",
    "            opt.apply_gradients(zip(alpha_grads, [alpha]))\n",
    "\n",
    "            # update weights of critic net\n",
    "            for t_ws, ws in zip(critic1_target_net.weights, critic1_net.weights):\n",
    "                t_ws.assign((1-TAU)*t_ws + TAU*ws)\n",
    "\n",
    "            for t_ws, ws in zip(critic2_target_net.weights, critic2_net.weights):\n",
    "                t_ws.assign((1-TAU)*t_ws + TAU*ws)\n",
    "\n",
    "        observation = new_observation.copy()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # report losses after every episode\n",
    "    if episode%1 == 0 and replay_buffer.is_enough:\n",
    "        logger.info(f'episode:{episode}, critic loss = {c1_l}, {c2_l}, actor loss = {a_l}, sum_r={tf.reduce_sum(reward_list)}')\n",
    "        if (br:= tf.reduce_sum(reward_list)) > best_reward:\n",
    "            # save the weights which produces the best result\n",
    "            best_reward = br\n",
    "            logger.warning(f'The weight of the best reward {best_reward} has been saved')\n",
    "            for ws, wsn in zip((critic1_net, critic1_target_net, critic2_net, critic2_target_net, actor_net), ('critic1_net', 'critic1_target_net', 'critic2_net', 'critic2_target_net', 'actor_net')):\n",
    "                ws.save_weights(filepath=f'{path}/{wsn}')\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "182967ef90c96c3dd142793ad3099ce97eb8a262f09752ff7be3d3d6d379c1b1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
