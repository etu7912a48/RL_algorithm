{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\tensorflow\\__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils as _distutils\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from my_rl.agent import Agent, ReplayBuffer, Algorithm\n",
    "from my_rl.algorithms.sac import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "d:\\113403\\project\\DQL\\venv\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "game_name = 'BipedalWalker-v3'\n",
    "\n",
    "# create env of the gam\n",
    "env = gym.make(game_name, render_mode=None)\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "action_scale = (env.action_space.high[0] - env.action_space.low[0])/2\n",
    "\n",
    "# create agent\n",
    "rb = ReplayBuffer(capacity=10000)\n",
    "alg = Algorithm(algorithm=SAC(action_dims=action_space))\n",
    "agent = Agent(replay_buffer=rb, algorithm=alg, action_scale=action_scale, state_space=state_space, action_space=action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\113403\\project\\DQL\\my_rl\\agent.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.memory[x] for x in batch_index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:47, avg_al=1.084346890449524 sum_r=-143.51987276325644\n",
      "episode:48, avg_al=0.5811080932617188 sum_r=-95.50332801712057\n",
      "episode:49, avg_al=0.5559979677200317 sum_r=-119.47039509302408\n",
      "episode:50, avg_al=0.6238800883293152 sum_r=-98.98048688071097\n",
      "episode:51, avg_al=0.597909152507782 sum_r=-101.4786672308746\n",
      "episode:52, avg_al=0.644525945186615 sum_r=-98.5977357846008\n",
      "episode:53, avg_al=0.7515549659729004 sum_r=-98.46859999090806\n",
      "episode:54, avg_al=0.7453850507736206 sum_r=-121.41882365320637\n",
      "episode:55, avg_al=0.929786205291748 sum_r=-100.10541974299727\n"
     ]
    }
   ],
   "source": [
    "agent.replay_buffer.clear_memory()\n",
    "\n",
    "for episode in range(1000):\n",
    "    observation, _ = env.reset(return_info=True)\n",
    "    al_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    for step in range(1000):\n",
    "        action = agent.sample_action(observation.reshape(1, state_space))\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        reward_list.append(reward)\n",
    "        done = tf.cast(done, tf.float32)\n",
    "        agent.replay_buffer.add_data((tf.squeeze(observation), action, reward, done, tf.squeeze(new_observation)))\n",
    "\n",
    "        if agent.replay_buffer.enough_data:\n",
    "            loss = agent.train()\n",
    "            al_list.append(loss)\n",
    "\n",
    "        observation = new_observation.copy()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # report losses after every episode\n",
    "    if episode%1 == 0 and agent.replay_buffer.enough_data:\n",
    "        print(f'episode:{episode}, avg_al={tf.reduce_mean(al_list)} sum_r={tf.reduce_sum(reward_list)}')\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "182967ef90c96c3dd142793ad3099ce97eb8a262f09752ff7be3d3d6d379c1b1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
